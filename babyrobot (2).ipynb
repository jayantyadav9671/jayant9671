{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfwODzEXNrZg",
        "outputId": "84316ab9-409c-4404-d4e3-0886cc2623e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Reward Values: [0.44117647 0.2        0.475      0.69565217 0.10526316]\n",
            "Total Reward: 662\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "estimates = np.zeros(num_arms)\n",
        "arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "reward_history = []\n",
        "\n",
        "for step in range(num_steps):\n",
        "    chosen_arm = choose_arm(epsilon, estimates)\n",
        "    arm_selection_counts[chosen_arm] += 1\n",
        "    reward = np.random.binomial(1, true_rewards[chosen_arm] / 10)  # Use true_rewards as mean of a Bernoulli distribution\n",
        "    estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "    reward_history.append(reward)\n",
        "\n",
        "print(\"Estimated Reward Values:\", estimates)\n",
        "\n",
        "total_reward = np.sum(reward_history)\n",
        "print(\"Total Reward:\", total_reward)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ4_emooUqp8",
        "outputId": "8dc9726a-a66a-42ca-b11b-f70b86614b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative Mean Rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.14285714285714285, 0.125, 0.2222222222222222, 0.3, 0.36363636363636365, 0.3333333333333333, 0.38461538461538464, 0.35714285714285715, 0.4, 0.4375, 0.4117647058823529, 0.4444444444444444, 0.47368421052631576, 0.45, 0.47619047619047616, 0.45454545454545453, 0.43478260869565216, 0.4583333333333333, 0.48, 0.5, 0.48148148148148145, 0.4642857142857143, 0.4482758620689655, 0.4666666666666667, 0.4838709677419355, 0.5, 0.48484848484848486, 0.5, 0.4857142857142857, 0.4722222222222222, 0.4594594594594595, 0.4473684210526316, 0.46153846153846156, 0.45, 0.4634146341463415, 0.47619047619047616, 0.4883720930232558, 0.4772727272727273, 0.4888888888888889, 0.5, 0.48936170212765956, 0.4791666666666667, 0.46938775510204084, 0.48, 0.47058823529411764, 0.46153846153846156, 0.4716981132075472, 0.48148148148148145, 0.4727272727272727, 0.48214285714285715, 0.49122807017543857, 0.4827586206896552, 0.4915254237288136, 0.48333333333333334, 0.4918032786885246, 0.4838709677419355, 0.49206349206349204, 0.5, 0.5076923076923077, 0.5151515151515151, 0.5074626865671642, 0.5147058823529411, 0.5072463768115942, 0.5, 0.49295774647887325, 0.5, 0.5068493150684932, 0.5135135135135135, 0.52, 0.5263157894736842, 0.5324675324675324, 0.5384615384615384, 0.5443037974683544, 0.5375, 0.5308641975308642, 0.5365853658536586, 0.5301204819277109, 0.5357142857142857, 0.5411764705882353, 0.5348837209302325, 0.5287356321839081, 0.5227272727272727, 0.5280898876404494, 0.5333333333333333, 0.5274725274725275, 0.5217391304347826, 0.5161290322580645, 0.5212765957446809, 0.5263157894736842, 0.5208333333333334, 0.5257731958762887, 0.5306122448979592, 0.5353535353535354, 0.53, 0.5247524752475248, 0.5294117647058824, 0.5339805825242718, 0.5384615384615384, 0.5428571428571428, 0.5471698113207547, 0.5420560747663551, 0.5462962962962963, 0.5412844036697247, 0.5454545454545454, 0.5495495495495496, 0.5446428571428571, 0.5398230088495575, 0.543859649122807, 0.5478260869565217, 0.5517241379310345, 0.5470085470085471, 0.5423728813559322, 0.5462184873949579, 0.55, 0.5537190082644629, 0.5573770491803278, 0.5609756097560976, 0.5564516129032258, 0.56, 0.5634920634920635, 0.5669291338582677, 0.5703125, 0.5736434108527132, 0.5692307692307692, 0.5725190839694656, 0.5757575757575758, 0.5789473684210527, 0.582089552238806, 0.5851851851851851, 0.5882352941176471, 0.5912408759124088, 0.5869565217391305, 0.5899280575539568, 0.5928571428571429, 0.5886524822695035, 0.5915492957746479, 0.5874125874125874, 0.5902777777777778, 0.593103448275862, 0.589041095890411, 0.5850340136054422, 0.5878378378378378, 0.5906040268456376, 0.5933333333333334, 0.5960264900662252, 0.5986842105263158, 0.5947712418300654, 0.5909090909090909, 0.5935483870967742, 0.5961538461538461, 0.5923566878980892, 0.5949367088607594, 0.5911949685534591, 0.59375, 0.5900621118012422, 0.5864197530864198, 0.588957055214724, 0.5853658536585366, 0.5878787878787879, 0.5903614457831325, 0.592814371257485, 0.5952380952380952, 0.5976331360946746, 0.6, 0.6023391812865497, 0.5988372093023255, 0.6011560693641619, 0.603448275862069, 0.6, 0.6022727272727273, 0.6045197740112994, 0.6067415730337079, 0.6089385474860335, 0.6111111111111112, 0.6132596685082873, 0.6153846153846154, 0.6120218579234973, 0.6086956521739131, 0.6108108108108108, 0.6075268817204301, 0.6096256684491979, 0.6063829787234043, 0.6084656084656085, 0.6105263157894737, 0.612565445026178, 0.6145833333333334, 0.616580310880829, 0.6134020618556701, 0.6153846153846154, 0.6173469387755102, 0.6142131979695431, 0.6161616161616161, 0.6180904522613065, 0.62, 0.6218905472636815, 0.6237623762376238, 0.6206896551724138, 0.6225490196078431, 0.624390243902439, 0.6213592233009708, 0.6231884057971014, 0.6201923076923077, 0.6220095693779905, 0.6190476190476191, 0.6208530805687204, 0.6226415094339622, 0.6244131455399061, 0.6214953271028038, 0.6186046511627907, 0.6203703703703703, 0.6221198156682027, 0.6192660550458715, 0.6210045662100456, 0.6227272727272727, 0.6244343891402715, 0.6261261261261262, 0.6278026905829597, 0.6294642857142857, 0.6266666666666667, 0.6238938053097345, 0.6211453744493393, 0.6228070175438597, 0.6200873362445415, 0.6217391304347826, 0.6233766233766234, 0.625, 0.6223175965665236, 0.6196581196581197, 0.6212765957446809, 0.6186440677966102, 0.620253164556962, 0.6218487394957983, 0.6234309623430963, 0.625, 0.6224066390041494, 0.6198347107438017, 0.6213991769547325, 0.6229508196721312, 0.6244897959183674, 0.6260162601626016, 0.6275303643724697, 0.625, 0.6265060240963856, 0.624, 0.6254980079681275, 0.626984126984127, 0.6245059288537549, 0.6220472440944882, 0.6235294117647059, 0.625, 0.6264591439688716, 0.624031007751938, 0.6216216216216216, 0.6192307692307693, 0.6206896551724138, 0.6221374045801527, 0.6197718631178707, 0.6212121212121212, 0.6226415094339622, 0.6203007518796992, 0.6179775280898876, 0.6194029850746269, 0.620817843866171, 0.6222222222222222, 0.6199261992619927, 0.6213235294117647, 0.6190476190476191, 0.6204379562043796, 0.6218181818181818, 0.6231884057971014, 0.6209386281588448, 0.6223021582733813, 0.6236559139784946, 0.6214285714285714, 0.6192170818505338, 0.6205673758865248, 0.6219081272084805, 0.6232394366197183, 0.624561403508772, 0.6258741258741258, 0.6236933797909407, 0.6215277777777778, 0.6193771626297578, 0.6172413793103448, 0.6151202749140894, 0.6164383561643836, 0.6143344709897611, 0.6122448979591837, 0.6135593220338983, 0.6148648648648649, 0.6161616161616161, 0.6140939597315436, 0.6120401337792643, 0.6133333333333333, 0.6146179401993356, 0.6158940397350994, 0.6171617161716172, 0.618421052631579, 0.6196721311475409, 0.6209150326797386, 0.6221498371335505, 0.6233766233766234, 0.6245954692556634, 0.6258064516129033, 0.6270096463022508, 0.6282051282051282, 0.6293929712460063, 0.6305732484076433, 0.6317460317460317, 0.6329113924050633, 0.6309148264984227, 0.6320754716981132, 0.6300940438871473, 0.63125, 0.632398753894081, 0.6335403726708074, 0.6346749226006192, 0.6327160493827161, 0.6338461538461538, 0.6349693251533742, 0.636085626911315, 0.6341463414634146, 0.6352583586626139, 0.6363636363636364, 0.6374622356495468, 0.6385542168674698, 0.6396396396396397, 0.6407185628742516, 0.6417910447761194, 0.6428571428571429, 0.6439169139465876, 0.6420118343195266, 0.6430678466076696, 0.6441176470588236, 0.6451612903225806, 0.6461988304093568, 0.6472303206997084, 0.6482558139534884, 0.6492753623188405, 0.6502890173410405, 0.6484149855907781, 0.646551724137931, 0.6475644699140402, 0.6485714285714286, 0.6467236467236467, 0.6448863636363636, 0.6458923512747875, 0.6440677966101694, 0.6450704225352113, 0.6432584269662921, 0.6414565826330533, 0.6424581005586593, 0.6434540389972145, 0.6444444444444445, 0.6454293628808865, 0.6464088397790055, 0.6446280991735537, 0.6456043956043956, 0.6438356164383562, 0.644808743169399, 0.6457765667574932, 0.6467391304347826, 0.6449864498644986, 0.6459459459459459, 0.6469002695417789, 0.6451612903225806, 0.646112600536193, 0.6443850267379679, 0.6453333333333333, 0.6462765957446809, 0.6472148541114059, 0.6481481481481481, 0.6490765171503958, 0.65, 0.6509186351706037, 0.6518324607329843, 0.6527415143603134, 0.6536458333333334, 0.6519480519480519, 0.6502590673575129, 0.6511627906976745, 0.6494845360824743, 0.6503856041131105, 0.6487179487179487, 0.6470588235294118, 0.6479591836734694, 0.648854961832061, 0.649746192893401, 0.6506329113924051, 0.648989898989899, 0.6473551637279596, 0.6482412060301508, 0.6491228070175439, 0.65, 0.6508728179551122, 0.6517412935323383, 0.652605459057072, 0.650990099009901, 0.6493827160493827, 0.6502463054187192, 0.6511056511056511, 0.6519607843137255, 0.6503667481662592, 0.651219512195122, 0.6496350364963503, 0.6480582524271845, 0.648910411622276, 0.6473429951690821, 0.6481927710843374, 0.6490384615384616, 0.6474820143884892, 0.645933014354067, 0.6467780429594272, 0.6476190476190476, 0.6484560570071259, 0.6469194312796208, 0.6477541371158393, 0.6485849056603774, 0.6470588235294118, 0.6455399061032864, 0.6440281030444965, 0.6448598130841121, 0.6433566433566433, 0.6441860465116279, 0.642691415313225, 0.6412037037037037, 0.6420323325635104, 0.6428571428571429, 0.6436781609195402, 0.6444954128440367, 0.6430205949656751, 0.6438356164383562, 0.6446469248291572, 0.6454545454545455, 0.6462585034013606, 0.6470588235294118, 0.6478555304740407, 0.6486486486486487, 0.647191011235955, 0.6457399103139013, 0.6442953020134228, 0.6450892857142857, 0.643652561247216, 0.6422222222222222, 0.6407982261640798, 0.6415929203539823, 0.6401766004415012, 0.6409691629955947, 0.6395604395604395, 0.6381578947368421, 0.6367614879649891, 0.6375545851528385, 0.6383442265795207, 0.6369565217391304, 0.6377440347071583, 0.6385281385281385, 0.6393088552915767, 0.6400862068965517, 0.6387096774193548, 0.6373390557939914, 0.6381156316916489, 0.6367521367521367, 0.6375266524520256, 0.6382978723404256, 0.6390658174097664, 0.6398305084745762, 0.6405919661733616, 0.6413502109704642, 0.6421052631578947, 0.6407563025210085, 0.6415094339622641, 0.6401673640167364, 0.6409185803757829, 0.6416666666666667, 0.6403326403326404, 0.6410788381742739, 0.6418219461697723, 0.6425619834710744, 0.6412371134020619, 0.6419753086419753, 0.6427104722792608, 0.6413934426229508, 0.6400817995910021, 0.6408163265306123, 0.6415478615071283, 0.6402439024390244, 0.640973630831643, 0.6417004048582996, 0.6424242424242425, 0.6431451612903226, 0.6438631790744467, 0.6445783132530121, 0.6432865731462926, 0.644, 0.6427145708582834, 0.6414342629482072, 0.6401590457256461, 0.6388888888888888, 0.6376237623762376, 0.6363636363636364, 0.6370808678500987, 0.6377952755905512, 0.6385068762278978, 0.6372549019607843, 0.6360078277886497, 0.63671875, 0.6354775828460039, 0.6342412451361867, 0.6349514563106796, 0.6356589147286822, 0.6363636363636364, 0.637065637065637, 0.6358381502890174, 0.6365384615384615, 0.6353166986564299, 0.6360153256704981, 0.6347992351816444, 0.6354961832061069, 0.6342857142857142, 0.6330798479087453, 0.6337760910815939, 0.6325757575757576, 0.6332703213610587, 0.6339622641509434, 0.6346516007532956, 0.6353383458646616, 0.6341463414634146, 0.6348314606741573, 0.6355140186915887, 0.6361940298507462, 0.6368715083798883, 0.6356877323420075, 0.6363636363636364, 0.6370370370370371, 0.6358595194085028, 0.6365313653136532, 0.6372007366482505, 0.6378676470588235, 0.6385321100917432, 0.6391941391941391, 0.6398537477148081, 0.6405109489051095, 0.639344262295082, 0.64, 0.6388384754990926, 0.6394927536231884, 0.6401446654611211, 0.6407942238267148, 0.6414414414414414, 0.6402877697841727, 0.6409335727109515, 0.6397849462365591, 0.6386404293381037, 0.6392857142857142, 0.6399286987522281, 0.6405693950177936, 0.6394316163410302, 0.6382978723404256, 0.6371681415929203, 0.6378091872791519, 0.63668430335097, 0.6355633802816901, 0.6344463971880492, 0.6350877192982456, 0.6339754816112084, 0.6346153846153846, 0.6352530541012217, 0.6341463414634146, 0.6330434782608696, 0.6336805555555556, 0.6325823223570191, 0.6332179930795848, 0.6338514680483592, 0.6344827586206897, 0.6333907056798623, 0.6323024054982818, 0.6312178387650086, 0.6318493150684932, 0.6324786324786325, 0.6313993174061433, 0.6303236797274276, 0.6309523809523809, 0.629881154499151, 0.6305084745762712, 0.6311336717428088, 0.6317567567567568, 0.6306913996627319, 0.6313131313131313, 0.6319327731092437, 0.6308724832214765, 0.6314907872696818, 0.6321070234113713, 0.6327212020033389, 0.6333333333333333, 0.6339434276206323, 0.6345514950166113, 0.6351575456053068, 0.6357615894039735, 0.6363636363636364, 0.636963696369637, 0.6375617792421746, 0.6365131578947368, 0.6371100164203612, 0.6360655737704918, 0.6350245499181669, 0.6356209150326797, 0.634584013050571, 0.6351791530944625, 0.6357723577235772, 0.6363636363636364, 0.6353322528363047, 0.6359223300970874, 0.6365105008077544, 0.635483870967742, 0.6360708534621579, 0.635048231511254, 0.6356340288924559, 0.6346153846153846, 0.6352, 0.634185303514377, 0.6347687400318979, 0.6353503184713376, 0.6359300476947536, 0.6365079365079365, 0.6370839936608558, 0.6360759493670886, 0.636650868878357, 0.637223974763407, 0.6377952755905512, 0.6383647798742138, 0.6389324960753532, 0.6394984326018809, 0.6384976525821596, 0.6390625, 0.6396255850234009, 0.6401869158878505, 0.640746500777605, 0.6413043478260869, 0.641860465116279, 0.6424148606811145, 0.6414219474497682, 0.6419753086419753, 0.6425269645608629, 0.6415384615384615, 0.642089093701997, 0.6426380368098159, 0.6431852986217458, 0.6437308868501529, 0.6427480916030535, 0.6417682926829268, 0.6423135464231354, 0.6413373860182371, 0.6418816388467374, 0.6424242424242425, 0.642965204236006, 0.6419939577039275, 0.6425339366515838, 0.6430722891566265, 0.643609022556391, 0.6441441441441441, 0.6446776611694153, 0.6452095808383234, 0.6457399103139013, 0.6447761194029851, 0.6438152011922503, 0.6443452380952381, 0.6433878157503715, 0.6439169139465876, 0.642962962962963, 0.643491124260355, 0.6425406203840472, 0.6430678466076696, 0.6435935198821797, 0.6441176470588236, 0.644640234948605, 0.6451612903225806, 0.6456808199121523, 0.6461988304093568, 0.6452554744525547, 0.6457725947521866, 0.6462882096069869, 0.6468023255813954, 0.6473149492017417, 0.6478260869565218, 0.6483357452966715, 0.6473988439306358, 0.6464646464646465, 0.6469740634005764, 0.6474820143884892, 0.6479885057471264, 0.648493543758967, 0.6475644699140402, 0.648068669527897, 0.6485714285714286, 0.6490727532097005, 0.6495726495726496, 0.6500711237553343, 0.6505681818181818, 0.6510638297872341, 0.6515580736543909, 0.652050919377652, 0.652542372881356, 0.6530324400564175, 0.6535211267605634, 0.6540084388185654, 0.6544943820224719, 0.6535764375876578, 0.65406162464986, 0.6545454545454545, 0.6550279329608939, 0.6555090655509066, 0.6559888579387186, 0.6564673157162726, 0.6555555555555556, 0.6560332871012483, 0.6551246537396122, 0.6542185338865837, 0.6546961325966851, 0.6537931034482759, 0.6528925619834711, 0.6533700137551581, 0.6524725274725275, 0.6529492455418381, 0.6520547945205479, 0.652530779753762, 0.6516393442622951, 0.6521145975443383, 0.6525885558583107, 0.6530612244897959, 0.6535326086956522, 0.6540027137042063, 0.6531165311653117, 0.6535859269282814, 0.654054054054054, 0.6545209176788124, 0.6549865229110512, 0.6541049798115747, 0.6545698924731183, 0.6536912751677852, 0.6541554959785523, 0.6546184738955824, 0.6550802139037433, 0.6542056074766355, 0.6546666666666666, 0.6551264980026631, 0.6555851063829787, 0.6547144754316069, 0.6551724137931034, 0.6556291390728477, 0.656084656084656, 0.6565389696169088, 0.6569920844327177, 0.6574440052700923, 0.656578947368421, 0.657030223390276, 0.6561679790026247, 0.6566186107470511, 0.6557591623036649, 0.6549019607843137, 0.6553524804177546, 0.6558018252933507, 0.6549479166666666, 0.6540962288686606, 0.6532467532467533, 0.6523994811932555, 0.6515544041450777, 0.6507115135834411, 0.6511627906976745, 0.6503225806451612, 0.6507731958762887, 0.6499356499356499, 0.6503856041131105, 0.6495507060333762, 0.6487179487179487, 0.649167733674776, 0.649616368286445, 0.6500638569604087, 0.6492346938775511, 0.6496815286624203, 0.6501272264631043, 0.6493011435832274, 0.6484771573604061, 0.6489226869455006, 0.6481012658227848, 0.6485461441213654, 0.6477272727272727, 0.648171500630517, 0.6486146095717884, 0.6490566037735849, 0.6494974874371859, 0.6499372647427855, 0.6491228070175439, 0.6483103879849812, 0.6475, 0.6479400749063671, 0.6483790523690773, 0.6488169364881694, 0.6492537313432836, 0.6484472049689441, 0.6476426799007444, 0.6480793060718711, 0.6485148514851485, 0.6489493201483313, 0.6493827160493827, 0.6498150431565968, 0.6490147783251231, 0.6482164821648216, 0.6486486486486487, 0.649079754601227, 0.6495098039215687, 0.6499388004895961, 0.6491442542787286, 0.6495726495726496, 0.65, 0.6504263093788063, 0.6496350364963503, 0.6500607533414338, 0.6492718446601942, 0.6484848484848484, 0.648910411622276, 0.6493349455864571, 0.6485507246376812, 0.6477683956574186, 0.6481927710843374, 0.6486161251504212, 0.6490384615384616, 0.6494597839135654, 0.6486810551558753, 0.6479041916167665, 0.6471291866028708, 0.6475507765830346, 0.6479713603818615, 0.6471990464839095, 0.6464285714285715, 0.6468489892984542, 0.6460807600950119, 0.6465005931198102, 0.6457345971563981, 0.6461538461538462, 0.6465721040189125, 0.6458087367178277, 0.6462264150943396, 0.6466431095406361, 0.6470588235294118, 0.6474735605170387, 0.6467136150234741, 0.6459554513481829, 0.6463700234192038, 0.6467836257309941, 0.647196261682243, 0.646441073512252, 0.6468531468531469, 0.6472642607683353, 0.6476744186046511, 0.6480836236933798, 0.648491879350348, 0.6477404403244496, 0.6481481481481481, 0.6485549132947976, 0.6478060046189377, 0.6482122260668973, 0.6474654377880185, 0.6478711162255466, 0.6482758620689655, 0.6486796785304249, 0.6490825688073395, 0.6483390607101948, 0.648741418764302, 0.6491428571428571, 0.6484018264840182, 0.6476624857468644, 0.6480637813211845, 0.6473265073947668, 0.6477272727272727, 0.6469920544835415, 0.6473922902494331, 0.6477916194790487, 0.6470588235294118, 0.6463276836158192, 0.6467268623024831, 0.6471251409244645, 0.6475225225225225, 0.6479190101237345, 0.648314606741573, 0.6475869809203143, 0.647982062780269, 0.6483762597984323, 0.6476510067114094, 0.6480446927374302, 0.6484375, 0.6477146042363434, 0.6469933184855234, 0.6473859844271412, 0.6477777777777778, 0.6481687014428413, 0.6485587583148559, 0.6489479512735327, 0.6493362831858407, 0.6497237569060773, 0.6501103752759382, 0.649393605292172, 0.6497797356828194, 0.6501650165016502, 0.6494505494505495, 0.6498353457738749, 0.6502192982456141, 0.6495071193866374, 0.649890590809628, 0.6502732240437158, 0.6506550218340611, 0.6499454743729552, 0.6492374727668845, 0.6485310119695321, 0.6478260869565218, 0.6482084690553745, 0.6475054229934925, 0.647887323943662, 0.6471861471861472, 0.6475675675675676, 0.6479481641468683, 0.6472491909385113, 0.646551724137931, 0.6469321851453176, 0.6473118279569893, 0.6476906552094522, 0.648068669527897, 0.647374062165059, 0.6466809421841542, 0.6459893048128342, 0.6452991452991453, 0.6456776947705443, 0.6460554371002132, 0.645367412140575, 0.6457446808510638, 0.6450584484590861, 0.6443736730360934, 0.6447507953340403, 0.6451271186440678, 0.6455026455026455, 0.645877378435518, 0.6451953537486801, 0.6445147679324894, 0.6448893572181243, 0.6452631578947369, 0.6456361724500526, 0.6460084033613446, 0.646379853095488, 0.6467505241090147, 0.6460732984293194, 0.6464435146443515, 0.6468129571577848, 0.6471816283924844, 0.6475495307612096, 0.6479166666666667, 0.6472424557752341, 0.6465696465696466, 0.6469366562824507, 0.6473029045643154, 0.6476683937823834, 0.6480331262939959, 0.6483971044467425, 0.6487603305785123, 0.6491228070175439, 0.6494845360824743, 0.6498455200823893, 0.6502057613168725, 0.6495375128468653, 0.648870636550308, 0.6482051282051282, 0.6485655737704918, 0.6489252814738997, 0.6492842535787321, 0.6486210418794689, 0.6479591836734694, 0.6483180428134556, 0.6486761710794298, 0.6480162767039674, 0.6483739837398373, 0.6487309644670051, 0.6490872210953347, 0.6494427558257345, 0.6497975708502024, 0.6491405460060667, 0.6484848484848484, 0.6488395560040363, 0.6491935483870968, 0.649546827794562, 0.6498993963782697, 0.6492462311557788, 0.6495983935742972, 0.649949849548646, 0.6492985971943888, 0.6496496496496497, 0.65]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "estimates = np.zeros(num_arms)\n",
        "arm_selection_counts = np.zeros(num_arms)\n",
        "cumulative_reward = 0\n",
        "cumulative_mean_rewards = []\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "for step in range(num_steps):\n",
        "    chosen_arm = choose_arm(epsilon, estimates)\n",
        "    arm_selection_counts[chosen_arm] += 1\n",
        "    reward = np.random.binomial(1, true_rewards[chosen_arm] / 10)  # Use true_rewards as mean of a Bernoulli distribution\n",
        "    cumulative_reward += reward\n",
        "    estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "    cumulative_mean_rewards.append(cumulative_reward / (step + 1))\n",
        "\n",
        "print(\"Cumulative Mean Rewards:\", cumulative_mean_rewards)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upKk8nb9VRI_",
        "outputId": "97d41e3a-0c63-45f5-8061-9f375facdc8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Total Reward across Trials: 420.519\n",
            "Average Cumulative Mean Rewards across Trials: [0.42       0.422      0.41533333 0.42275    0.4238     0.42516667\n",
            " 0.42471429 0.4215     0.42111111 0.4216     0.42036364 0.42075\n",
            " 0.42092308 0.42021429 0.4188     0.418375   0.41858824 0.41777778\n",
            " 0.41726316 0.4171     0.41690476 0.41781818 0.41808696 0.41825\n",
            " 0.41852    0.41896154 0.41862963 0.41885714 0.41965517 0.4197\n",
            " 0.41974194 0.41946875 0.42021212 0.42035294 0.41985714 0.41963889\n",
            " 0.42002703 0.41965789 0.41971795 0.419875   0.42065854 0.4207381\n",
            " 0.4205814  0.42018182 0.41986667 0.42030435 0.4197234  0.41939583\n",
            " 0.41902041 0.4191     0.41952941 0.41976923 0.41981132 0.4202963\n",
            " 0.42021818 0.41998214 0.42029825 0.42012069 0.42022034 0.42011667\n",
            " 0.42008197 0.42037097 0.4204127  0.42075    0.42093846 0.42133333\n",
            " 0.42134328 0.42129412 0.42143478 0.42122857 0.42128169 0.42126389\n",
            " 0.42126027 0.42144595 0.42132    0.42102632 0.42045455 0.42084615\n",
            " 0.42094937 0.4209625  0.4205679  0.4205     0.42063855 0.42054762\n",
            " 0.42025882 0.42039535 0.42025287 0.42022727 0.42029213 0.4201\n",
            " 0.42027473 0.42006522 0.41984946 0.41987234 0.41995789 0.41954167\n",
            " 0.41968041 0.41939796 0.41960606 0.41977    0.41953465 0.41954902\n",
            " 0.41968932 0.41966346 0.4195619  0.41951887 0.41939252 0.41958333\n",
            " 0.41956881 0.41973636 0.41965766 0.41976786 0.41980531 0.41963158\n",
            " 0.41964348 0.41969828 0.41979487 0.41977966 0.41987395 0.41970833\n",
            " 0.41997521 0.41990984 0.41991057 0.41993548 0.419976   0.42002381\n",
            " 0.42014173 0.42021875 0.42004651 0.4198     0.4199084  0.42012121\n",
            " 0.4202782  0.42027612 0.42020741 0.42017647 0.42022628 0.42018116\n",
            " 0.42010791 0.41994286 0.42002128 0.42000704 0.41994406 0.42002778\n",
            " 0.41988966 0.41985616 0.41984354 0.41977703 0.41979866 0.41977333\n",
            " 0.41961589 0.41953289 0.41948366 0.41957143 0.41966452 0.41988462\n",
            " 0.41993631 0.41986076 0.41983019 0.42009375 0.42020497 0.42018519\n",
            " 0.42008589 0.42006098 0.4203697  0.42059639 0.42071257 0.42061905\n",
            " 0.42046154 0.4205     0.42053801 0.42057558 0.42079191 0.42077011\n",
            " 0.42084571 0.42081818 0.42077966 0.42075843 0.42076536 0.42068889\n",
            " 0.42069613 0.42081868 0.42084699 0.42075    0.42067027 0.4206828\n",
            " 0.42066845 0.42057447 0.42066138 0.42061053 0.4205288  0.420375\n",
            " 0.42032642 0.42037629 0.42027692 0.42013265 0.42021827 0.42022222\n",
            " 0.42028141 0.420295   0.42027363 0.42031683 0.42025123 0.42027941\n",
            " 0.42030732 0.42031068 0.42030918 0.42032212 0.42032536 0.42040476\n",
            " 0.42034123 0.42034906 0.42030516 0.42040187 0.42046047 0.42056019\n",
            " 0.42057604 0.42052294 0.42059361 0.42057727 0.42061538 0.42068919\n",
            " 0.4206861  0.42065625 0.42063556 0.42050442 0.42063877 0.42070175\n",
            " 0.42065502 0.42066522 0.42066234 0.42054741 0.42047639 0.42044017\n",
            " 0.42049362 0.4205339  0.4205865  0.4205042  0.42055649 0.42054167\n",
            " 0.42059751 0.42049587 0.42045267 0.4204959  0.42042041 0.42043496\n",
            " 0.42040081 0.42038306 0.42044177 0.420388   0.42040239 0.42038492\n",
            " 0.42042688 0.42042126 0.42058039 0.42053516 0.42051362 0.42046124\n",
            " 0.42044015 0.4204     0.42037931 0.42041603 0.42045627 0.42044318\n",
            " 0.42051698 0.42047368 0.4204382  0.42052985 0.42064684 0.42066667\n",
            " 0.42049815 0.42056985 0.420663   0.42059489 0.42058182 0.42052899\n",
            " 0.42057762 0.42055036 0.42053763 0.42056786 0.42049466 0.42040071\n",
            " 0.42046643 0.42046479 0.42050175 0.42056294 0.42055052 0.42059375\n",
            " 0.42072664 0.42074483 0.4205945  0.42063014 0.42060751 0.42055102\n",
            " 0.42058983 0.42052703 0.42052189 0.42049664 0.42046154 0.42048333\n",
            " 0.42042857 0.42044371 0.42045215 0.42030592 0.42041311 0.42035621\n",
            " 0.42034853 0.42025    0.4201521  0.42017097 0.42013183 0.42013782\n",
            " 0.42014696 0.42011783 0.42010159 0.42016456 0.4201388  0.42010692\n",
            " 0.42006897 0.42004063 0.42004361 0.41999689 0.42001548 0.42005556\n",
            " 0.42003077 0.4200092  0.41999694 0.42002439 0.42000608 0.41999697\n",
            " 0.42001813 0.42003916 0.42004505 0.42007784 0.42008955 0.42008333\n",
            " 0.4201543  0.42008284 0.42013274 0.42011471 0.42006158 0.42004678\n",
            " 0.42009621 0.420125   0.42016232 0.42023121 0.42031988 0.42034483\n",
            " 0.42040401 0.42038571 0.42042165 0.42039205 0.42033144 0.42035593\n",
            " 0.42036338 0.42036236 0.42036415 0.42028771 0.42023677 0.42022778\n",
            " 0.4201385  0.42008564 0.42004408 0.42001099 0.42001644 0.41999727\n",
            " 0.41998093 0.41988859 0.41994038 0.41994865 0.41997574 0.41995161\n",
            " 0.41993298 0.41995722 0.41995733 0.41993351 0.4198992  0.41990741\n",
            " 0.41993668 0.41994474 0.4199685  0.41995288 0.41995561 0.41991406\n",
            " 0.41990909 0.41989119 0.41983979 0.4198634  0.41997429 0.4199\n",
            " 0.41997442 0.42001786 0.41996438 0.42003807 0.42011392 0.42012374\n",
            " 0.42014861 0.42008543 0.42005263 0.42001    0.41997257 0.41994527\n",
            " 0.41996774 0.41988614 0.4199037  0.41987438 0.41989189 0.41991422\n",
            " 0.41994866 0.41998293 0.4200438  0.42004369 0.41998547 0.41999275\n",
            " 0.42000964 0.42001683 0.41994245 0.41994498 0.41993079 0.41992857\n",
            " 0.41997387 0.41995972 0.41997163 0.41999292 0.42001412 0.42007042\n",
            " 0.42006557 0.42001869 0.42005128 0.4200907  0.42003248 0.42005093\n",
            " 0.42001386 0.42004378 0.42001609 0.42001606 0.42004348 0.42008676\n",
            " 0.42011617 0.42010909 0.42017914 0.42019683 0.42019639 0.42017342\n",
            " 0.42017753 0.42016143 0.42014094 0.42012277 0.42012918 0.42008444\n",
            " 0.42005543 0.42008186 0.42011921 0.42014317 0.42014505 0.4200943\n",
            " 0.42015974 0.42015502 0.42017865 0.42020652 0.42018438 0.42018182\n",
            " 0.42017927 0.42019828 0.42015269 0.42019313 0.42015632 0.42017094\n",
            " 0.42020682 0.42020426 0.42016985 0.42016314 0.42019027 0.42014346\n",
            " 0.42016421 0.42014286 0.42012159 0.42017155 0.42017537 0.42018958\n",
            " 0.42020998 0.42020954 0.42025259 0.42023554 0.42019588 0.42017695\n",
            " 0.42016016 0.42014959 0.42012679 0.42014898 0.42009369 0.42002642\n",
            " 0.42006085 0.42005061 0.42007879 0.42008669 0.42003823 0.42003213\n",
            " 0.42002004 0.419988   0.42002196 0.41999801 0.41999602 0.41997222\n",
            " 0.42001584 0.42000395 0.41996844 0.41998031 0.4199666  0.41994314\n",
            " 0.41997652 0.41998438 0.4200039  0.42000778 0.41997476 0.41996705\n",
            " 0.41998646 0.41994208 0.41991329 0.41994615 0.41992322 0.41990996\n",
            " 0.41989866 0.41990649 0.41990286 0.41988403 0.41989943 0.4198447\n",
            " 0.41983365 0.41986604 0.41988701 0.41992105 0.4199606  0.41994007\n",
            " 0.41989159 0.41992351 0.41994972 0.41996097 0.42000186 0.41998704\n",
            " 0.41997412 0.41995018 0.41997422 0.41997426 0.41999083 0.42\n",
            " 0.42006399 0.42008759 0.42003097 0.42006364 0.42009074 0.42012138\n",
            " 0.42006148 0.42003791 0.4201009  0.42010252 0.42010772 0.42010932\n",
            " 0.42017174 0.4201875  0.42018538 0.42014591 0.4201421  0.42015248\n",
            " 0.42013982 0.42009541 0.42010935 0.42013556 0.42015817 0.42019298\n",
            " 0.42019965 0.42020629 0.42021466 0.42022997 0.42025217 0.42028993\n",
            " 0.42034315 0.42033045 0.42033333 0.42040172 0.42037005 0.42037629\n",
            " 0.42038422 0.42039726 0.42039829 0.42040273 0.42043782 0.42042517\n",
            " 0.42037351 0.42036271 0.42037902 0.42039527 0.42041147 0.42039899\n",
            " 0.4203916  0.42039933 0.42041709 0.42038963 0.42043406 0.42038667\n",
            " 0.42037604 0.42040365 0.42045274 0.42042715 0.42039339 0.42039109\n",
            " 0.42040362 0.42043914 0.42041872 0.42043279 0.42045663 0.42043791\n",
            " 0.42045677 0.42044788 0.4204     0.42037825 0.42037439 0.42036084\n",
            " 0.42034572 0.42032742 0.42032689 0.42036817 0.4203756  0.42033333\n",
            " 0.420304   0.42034026 0.42037799 0.42043949 0.42044992 0.42049841\n",
            " 0.420542   0.42057278 0.42059874 0.42057098 0.42054331 0.42058333\n",
            " 0.42059655 0.42059718 0.42056025 0.42051719 0.42050546 0.4205405\n",
            " 0.42052411 0.42056056 0.42057364 0.42058359 0.4206507  0.42066049\n",
            " 0.4206302  0.42062923 0.42060215 0.42061503 0.42062328 0.42060398\n",
            " 0.42062137 0.42061738 0.42062405 0.42060942 0.42058574 0.42055909\n",
            " 0.42056278 0.42053776 0.42055053 0.42054518 0.4205188  0.42048498\n",
            " 0.42047376 0.42053593 0.42050822 0.42048358 0.42046945 0.42047619\n",
            " 0.42044725 0.42046291 0.42045185 0.42044822 0.42041507 0.42043068\n",
            " 0.42040648 0.42040147 0.4203906  0.42041642 0.42046852 0.42047222\n",
            " 0.42046131 0.42043878 0.42041339 0.42037791 0.42036865 0.42037536\n",
            " 0.42037916 0.42035694 0.42035498 0.42035159 0.42033669 0.42031609\n",
            " 0.42034146 0.42032951 0.42039199 0.42039429 0.42040799 0.42037179\n",
            " 0.42034708 0.42036648 0.42035603 0.42035269 0.42034512 0.4203404\n",
            " 0.42035261 0.4203662  0.42034459 0.42036517 0.42034081 0.42037535\n",
            " 0.42037203 0.42038128 0.42037936 0.42036351 0.42035605 0.42035\n",
            " 0.42036061 0.42036565 0.42032918 0.42034254 0.42035586 0.42034711\n",
            " 0.42036176 0.42036813 0.42037723 0.42038493 0.42038714 0.42039617\n",
            " 0.42041337 0.42043188 0.42040136 0.42042663 0.42043148 0.42041192\n",
            " 0.42043166 0.42041892 0.4204278  0.42040701 0.42041184 0.42042608\n",
            " 0.42038926 0.42040751 0.42040562 0.42037166 0.42035915 0.42035333\n",
            " 0.42033422 0.42034707 0.42035325 0.42033156 0.42033245 0.42030159\n",
            " 0.42027213 0.42026517 0.42028327 0.42028158 0.42030486 0.42029265\n",
            " 0.4202464  0.42027749 0.42030196 0.42032115 0.42032725 0.42032422\n",
            " 0.42031339 0.4203013  0.42030999 0.42027332 0.42025744 0.42024289\n",
            " 0.42021806 0.42022809 0.42019691 0.42019794 0.42022978 0.42024103\n",
            " 0.4202484  0.42024169 0.42024266 0.42023852 0.42024076 0.42022901\n",
            " 0.4201906  0.42018909 0.42021546 0.42016203 0.42016182 0.42017677\n",
            " 0.42015132 0.42015113 0.42015094 0.42015578 0.42016688 0.42014286\n",
            " 0.42013392 0.42013875 0.42012235 0.42012095 0.42014819 0.42014677\n",
            " 0.42017267 0.42018114 0.42019579 0.42021163 0.42021137 0.42022099\n",
            " 0.42023058 0.42023892 0.42023616 0.4202543  0.42025031 0.42025735\n",
            " 0.42023745 0.42022372 0.42023321 0.42025244 0.42025091 0.42026399\n",
            " 0.42029769 0.42029005 0.42027636 0.42027966 0.42026239 0.42023913\n",
            " 0.42021954 0.42020602 0.42020939 0.42024399 0.42027491 0.42026139\n",
            " 0.42028383 0.42028469 0.42030466 0.42030788 0.42031824 0.42031667\n",
            " 0.42029251 0.42028504 0.42033096 0.42034123 0.4203432  0.4203747\n",
            " 0.42039669 0.42040212 0.42036631 0.42037412 0.42035605 0.42038263\n",
            " 0.42039977 0.42041569 0.42043275 0.42043341 0.42043757 0.42047203\n",
            " 0.42046566 0.42046977 0.42045877 0.42045012 0.42042063 0.42041667\n",
            " 0.42039769 0.42039954 0.42039216 0.42041244 0.42040967 0.42042069\n",
            " 0.42039036 0.42036812 0.4203551  0.420373   0.420384   0.42040411\n",
            " 0.42040821 0.42038155 0.4203595  0.42036591 0.42038365 0.42037755\n",
            " 0.42036693 0.42037217 0.42036497 0.42037585 0.42038331 0.4203964\n",
            " 0.42039145 0.42039888 0.42038272 0.42038117 0.42039418 0.4204217\n",
            " 0.42041564 0.42041295 0.42041249 0.42041203 0.42041046 0.42042333\n",
            " 0.42039956 0.42038914 0.42040864 0.42041261 0.42039779 0.42039073\n",
            " 0.42041125 0.4204163  0.42040814 0.42041868 0.42042042 0.42040351\n",
            " 0.42042497 0.42043435 0.42040656 0.42038537 0.42038931 0.42040087\n",
            " 0.42039282 0.42042391 0.4204354  0.42043384 0.42044204 0.42042208\n",
            " 0.42042595 0.42043089 0.42042611 0.42041379 0.4203929  0.42039677\n",
            " 0.42039957 0.42038948 0.42039335 0.42040792 0.42042995 0.42042521\n",
            " 0.42042583 0.42039979 0.42042918 0.42044574 0.42047503 0.42049788\n",
            " 0.42047084 0.42048411 0.42049206 0.4205074  0.42049525 0.42052004\n",
            " 0.42051844 0.42051474 0.42054154 0.42052941 0.4205446  0.42055136\n",
            " 0.42056021 0.42053138 0.42049634 0.42050104 0.42049009 0.42048542\n",
            " 0.42052133 0.42050832 0.42049637 0.4204834  0.42047772 0.42049482\n",
            " 0.42049018 0.42045661 0.42045098 0.42044227 0.42043151 0.42043827\n",
            " 0.42045118 0.42043943 0.42043179 0.42042725 0.42045548 0.42043865\n",
            " 0.42043922 0.42041633 0.42042406 0.42043075 0.42043845 0.42044614\n",
            " 0.42046497 0.42047769 0.42048531 0.42048988 0.42051769 0.42050505\n",
            " 0.42052472 0.42052419 0.42052467 0.4205161  0.42051156 0.42050803\n",
            " 0.42051354 0.42051503 0.42052753 0.420519  ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [0.4, 0.3, 0.5, 0.7, 0.2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "total_rewards_per_trial = []\n",
        "cumulative_mean_rewards_per_trial = []\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "    cumulative_mean_rewards = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        chosen_arm = choose_arm(epsilon, estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "        cumulative_mean_rewards.append(cumulative_reward / (step + 1))\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    cumulative_mean_rewards_per_trial.append(cumulative_mean_rewards)\n",
        "\n",
        "average_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "# Calculate the average cumulative mean rewards after all trials\n",
        "average_cumulative_mean_rewards = np.mean(cumulative_mean_rewards_per_trial, axis=0)\n",
        "\n",
        "print(\"Average Total Reward across Trials:\", average_total_reward)\n",
        "print(\"Average Cumulative Mean Rewards across Trials:\", average_cumulative_mean_rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7l5pyRi1XAYu",
        "outputId": "ab8dfbd8-162c-4c69-f3cc-8942d1559170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Mean Rewards for Each Arm: [0.36187996 0.27114218 0.45189183 0.6319367  0.18038549]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "    def choose_arm(epsilon, estimates):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(num_arms)\n",
        "        else:\n",
        "            return np.argmax(estimates)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        chosen_arm = choose_arm(epsilon, estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm:\", estimated_mean_rewards_per_arm)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wah_fcfcKgg",
        "outputId": "b8588b8b-1ba1-43c8-b748-2348311ceca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Mean Rewards for Each Arm (Greedy Approach): [0.36109361 0.         0.         0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "\n",
        "        chosen_arm = np.argmax(estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (Greedy Approach):\", estimated_mean_rewards_per_arm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j3NmfR3dKiR",
        "outputId": "c1c953b6-072c-4760-d72c-ba23c111e698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Mean Rewards for Each Arm (Greedy Approach): [0.36142261 0.         0.         0.         0.        ]\n",
            "Mean Total Reward across Trials: 571.971\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "total_rewards_per_trial = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Greedy approach: Always choose the arm with the highest estimated mean reward\n",
        "        chosen_arm = np.argmax(estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "mean_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (Greedy Approach):\", estimated_mean_rewards_per_arm)\n",
        "print(\"Mean Total Reward across Trials:\", mean_total_reward)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWZO_j-Dd6_j",
        "outputId": "354dd24d-fb62-4678-91ce-d2a5d71f3289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Mean Rewards for Each Arm (ε-Greedy Approach): [0.35827549 0.26920319 0.44745848 0.6319367  0.18328038]\n",
            "Mean Total Reward across Trials: 941.402\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "total_rewards_per_trial = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        if np.random.rand() < epsilon:\n",
        "\n",
        "            chosen_arm = np.random.choice(num_arms)\n",
        "        else:\n",
        "\n",
        "            chosen_arm = np.argmax(estimates)\n",
        "\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "mean_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (ε-Greedy Approach):\", estimated_mean_rewards_per_arm)\n",
        "print(\"Mean Total Reward across Trials:\", mean_total_reward)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}